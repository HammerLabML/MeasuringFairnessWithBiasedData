{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from embedding import BertHuggingface\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-illinois",
   "metadata": {},
   "source": [
    "## Get the data and (new) classes\n",
    "\n",
    "For the reviewed data all matching raw titles were assigned. In the TITLE_JSON these raw titles are sorted by classes, including potential new classes marked with *\n",
    "It might make sense to merge these with some existing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOS_FILE_REVIEWED = \"data/BIOS_REVIEWED.pkl\"\n",
    "TITLE_JSON = \"data/title_lookup.json\"\n",
    "\n",
    "reviewed_classes = ['architect', 'surgeon', 'dentist', 'teacher', 'psychologist', 'nurse', 'photographer', 'physician', 'attorney', 'journalist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open(BIOS_FILE_REVIEWED, \"rb\")) as openfile:\n",
    "    full_data = raw_data = pickle.load(openfile)\n",
    "\n",
    "with open(TITLE_JSON, 'r') as j:\n",
    "    raw_titles_per_title = json.load(j)\n",
    "    \n",
    "classes = [key for key in raw_titles_per_title.keys() if not '-' in key]\n",
    "        \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup for titles given raw titles\n",
    "raw_title_lookup = {}\n",
    "for title, raw_titles in raw_titles_per_title.items():\n",
    "    if title[0] == '-':\n",
    "        continue\n",
    "    for rt in raw_titles:\n",
    "        if not rt in raw_title_lookup.keys():\n",
    "            raw_title_lookup.update({rt: [title]})\n",
    "        else:\n",
    "            raw_title_lookup[rt].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'auto_raw_title' and 'auto_title' refer to the titles automatically assigned by the crawler/preprocessing\n",
    "# 'raw_titles' is a list of raw titles that were explicitly labeled during review (only availalbe for reviewed, valid samples)\n",
    "# 'titles' refers to the classes derived from the 'raw_titles' (multi-class!, only available for reviewed, valid samples)\n",
    "# 'review' is 1 if the sample was reviewed\n",
    "# if a sample was reviewed, the 'valid' flag shows if it is a valid biography*, if not other informations might be missing!\n",
    "# 'style_valid' shows if a valid biography also matches the style (as opposed to texts that contain the information for our classification task but are not actual biographies)\n",
    "# 'raw_edited' contains the edited raw text (if any changes happened during review) - this also includes some changes due to formatting issues and otherwise mostly removal of few words at the end (e.g. \"Read more...\")\n",
    "# 'comment' contains either the comment automatically generated when filtering the dataset or the reviewers comment (usually when the datapoint is invalid)\n",
    "\n",
    "# * in the broadest sense, e.g. refering to an actual person, containing useful information after the first sentence\n",
    "\n",
    "full_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-translator",
   "metadata": {},
   "source": [
    "## Results of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rjobs = [entry for entry in full_data if entry['auto_title'] in reviewed_classes]\n",
    "print(\"got\", len(data_rjobs), \"samples for the jobs that were reviewed\")\n",
    "\n",
    "data_reviewed = [entry for entry in data_rjobs if entry['review'] == 1]\n",
    "data_not_reviewed = [entry for entry in data_rjobs if entry['review'] == 0]\n",
    "print(\"of those \", len(data_reviewed), \"have been reviewed\")\n",
    "\n",
    "data_valid = [entry for entry in data_reviewed if entry['valid'] == 1]\n",
    "print(\"of those \", len(data_valid), \"are valid\")\n",
    "\n",
    "data_style_valid = [entry for entry in data_reviewed if entry['style_valid'] == 1]\n",
    "data_style_invalid = [entry for entry in data_reviewed if entry['style_valid'] == 0]\n",
    "print(\"of those \", len(data_style_valid), \" are stylistically valid, \", len(data_style_invalid), \" are stylistically invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_title = [entry for entry in data_valid if entry['titles'] == '' or (type(entry['titles']) == list and len(entry['titles']) == 0)]\n",
    "data_one_title = [entry for entry in data_valid if type(entry['titles']) == list and len(entry['titles']) == 1]\n",
    "data_wrong_title = [entry for entry in data_valid if type(entry['titles']) == list and entry['auto_title'] not in entry['titles']]\n",
    "data_multi_title = [entry for entry in data_valid if type(entry['titles']) == list and len(entry['titles']) > 1]\n",
    "\n",
    "print(\"% without any title: \", len(data_no_title)/len(data_valid))\n",
    "print(\"% with exactly one title: \", len(data_one_title)/len(data_valid))\n",
    "print(\"% where the auto title was wrong: \", len(data_wrong_title)/len(data_valid))\n",
    "print(\"% with multiple titles: \", len(data_multi_title)/len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_res = {}\n",
    "# review statistics per job (auto label)\n",
    "for job in reviewed_classes:\n",
    "    reviewed = [entry for entry in data_reviewed if job == entry['auto_title']]\n",
    "    if len(reviewed) == 0:\n",
    "        print(\"no reviewed samples for \", job)\n",
    "        continue\n",
    "        \n",
    "    valid = [entry for entry in data_valid if job == entry['auto_title']]\n",
    "    review_res.update({job: {}})\n",
    "    review_res[job]['samples (total)'] = int(len([entry for entry in data_rjobs if entry['auto_title'] == job]))\n",
    "    review_res[job]['samples (reviewed)'] = len(reviewed)\n",
    "    review_res[job]['samples* (reviewed)'] = len([entry for entry in data_reviewed if job in entry['titles']])\n",
    "    review_res[job]['% valid'] = len(valid)/len(reviewed)\n",
    "    review_res[job]['% style valid'] = len([entry for entry in data_style_valid if job == entry['auto_title']])/len(valid)\n",
    "    review_res[job]['% one title'] = len([entry for entry in data_one_title if job == entry['auto_title']])/len(valid)\n",
    "    review_res[job]['% wrong title'] = len([entry for entry in data_wrong_title if job == entry['auto_title']])/len(valid)\n",
    "    review_res[job]['% multiple titles'] = len([entry for entry in data_multi_title if job == entry['auto_title']])/len(valid)\n",
    "\n",
    "#inv_review = {k: {job: review_res[job][k]} for k in review_res['nurse'].keys() for job in review_res.keys()}\n",
    "review_df = pd.DataFrame(data=review_res)\n",
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(df):\n",
    "    rows = df.index.values\n",
    "    cols = df.columns\n",
    "    header = \"\"\n",
    "    for col in cols:\n",
    "        header += \" & \"+col\n",
    "    header += \"\\\\\\\\\"\n",
    "    print(header)\n",
    "    for i in range(1, len(rows)):\n",
    "        row = rows[i]\n",
    "        for col in cols:\n",
    "            val = df.iloc[i][col]\n",
    "            if int(val) == val:\n",
    "                val = int(val)\n",
    "            else:\n",
    "                val = float(int(val*1000))/1000\n",
    "            row += \" & \" + str(val)\n",
    "        row += \"\\\\\\\\\"\n",
    "        print(row)\n",
    "df_to_latex(review_df.loc[:, ['architect', 'surgeon', 'dentist', 'teacher', 'psychologist']])\n",
    "print()\n",
    "print()\n",
    "df_to_latex(review_df.loc[:, ['nurse', 'photographer', 'physician', 'attorney', 'journalist']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-gazette",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-farmer",
   "metadata": {},
   "source": [
    "### Samples and gender ratio per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_plus_classes = reviewed_classes + ['*software architect', '*writer', '*researcher']\n",
    "\n",
    "# unsupervised: all samples, auto-label\n",
    "samples_auto = [len([sample for sample in data_reviewed if sample['auto_title'] == c]) for c in reviewed_classes]\n",
    "ratio_auto = [len([sample for sample in data_reviewed if sample['auto_title'] == c and sample['gender'] == 'F'])/samples_auto[i] for i, c in enumerate(reviewed_classes)]\n",
    "\n",
    "# reviewed: valid samples, annotated labels\n",
    "samples_review = [len([sample for sample in data_valid if c in sample['titles']]) for c in reviewed_plus_classes]\n",
    "ratio_review = [len([sample for sample in data_valid if c in sample['titles'] and sample['gender'] == 'F'])/samples_review[i] for i, c in enumerate(reviewed_plus_classes)]\n",
    "\n",
    "sorted_classes = reviewed_classes\n",
    "\n",
    "for i, c in enumerate(sorted_classes):\n",
    "    print(c, samples_review[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this for better layout (no overlapping labels, labels not being cut when saves?)\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# sort classes by no. samples (auto labels)\n",
    "n = len(reviewed_classes)\n",
    "sorted_classes = [x for _, x in sorted(zip(samples_auto, sorted_classes))]+reviewed_plus_classes[n:]\n",
    "ratio_auto = [x for _, x in sorted(zip(samples_auto, ratio_auto))]\n",
    "samples_review = [x for _, x in sorted(zip(samples_auto, samples_review[:n]))]+samples_review[n:]\n",
    "ratio_review = [x for _, x in sorted(zip(samples_auto, ratio_review[:n]))]+ratio_review[n:]\n",
    "samples_auto = sorted(samples_auto)\n",
    "\n",
    "\n",
    "x = np.arange(len(reviewed_classes))\n",
    "x2 = np.arange(len(reviewed_plus_classes))\n",
    "\n",
    "width = 0.35\n",
    "offset = width/2\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,10))\n",
    "rects = axes[0].bar(x - offset, samples_auto, width, label=\"Unsupervised\")\n",
    "rects2 = axes[0].bar(x2 + offset, samples_review, width, label=\"Reviewed\")\n",
    "axes[0].set_xticks(x2)\n",
    "axes[0].set_xticklabels(sorted_classes, rotation=90)\n",
    "axes[0].set_title(\"Samples per class\")\n",
    "axes[0].legend(loc=\"upper left\")\n",
    "\n",
    "rects = axes[1].bar(x - offset, ratio_auto, width, label=\"Unsupervised\")\n",
    "rects2 = axes[1].bar(x2 + offset, ratio_review, width, label=\"Reviewed\")\n",
    "axes[1].set_xticks(x2)\n",
    "axes[1].set_xticklabels(sorted_classes, rotation=90)\n",
    "axes[1].set_title(\"Female ratio per class\")\n",
    "axes[1].legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig('plots/samples_ratio.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this for better layout (no overlapping labels, labels not being cut when saves?)\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# sort classes by no. samples (auto labels)\n",
    "n = len(reviewed_classes)\n",
    "sorted_classes = [x for _, x in sorted(zip(samples_auto, sorted_classes))]+reviewed_plus_classes[n:]\n",
    "ratio_auto = [x for _, x in sorted(zip(samples_auto, ratio_auto))]\n",
    "samples_review = [x for _, x in sorted(zip(samples_auto, samples_review[:n]))]+samples_review[n:]\n",
    "ratio_review = [x for _, x in sorted(zip(samples_auto, ratio_review[:n]))]+ratio_review[n:]\n",
    "samples_auto = sorted(samples_auto)\n",
    "\n",
    "\n",
    "x = np.arange(len(reviewed_classes))\n",
    "x2 = np.arange(len(reviewed_plus_classes))\n",
    "\n",
    "width = 0.35\n",
    "offset = width/2\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,10))\n",
    "rects = axes.bar(x - offset, samples_auto, width, label=\"Unsupervised\")\n",
    "rects2 = axes.bar(x2 + offset, samples_review, width, label=\"Reviewed\")\n",
    "axes.set_xticks(x2)\n",
    "axes.set_xticklabels(sorted_classes, rotation=90)\n",
    "axes.set_title(\"Samples per class\")\n",
    "axes.legend(loc=\"upper left\")\n",
    "plt.savefig('plots/samples.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,10))\n",
    "rects = axes.bar(x - offset, ratio_auto, width, label=\"Unsupervised\")\n",
    "rects2 = axes.bar(x2 + offset, ratio_review, width, label=\"Reviewed\")\n",
    "axes.set_xticks(x2)\n",
    "axes.set_xticklabels(sorted_classes, rotation=90)\n",
    "axes.set_title(\"Female ratio per class\")\n",
    "axes.legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig('plots/ratio.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-sullivan",
   "metadata": {},
   "source": [
    "### Auto-labels vs. annotated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(gender):\n",
    "    cm = np.zeros((len(reviewed_classes), len(reviewed_plus_classes)+1))\n",
    "    count = np.zeros(len(reviewed_classes))\n",
    "\n",
    "    for sample in data_valid:\n",
    "        if not sample['gender'] == gender:\n",
    "            continue\n",
    "        cid = reviewed_classes.index(sample['auto_title'])\n",
    "        new_ids = [reviewed_plus_classes.index(c) if c in reviewed_plus_classes else 13 for c in sample['titles']]\n",
    "        for idx in new_ids:\n",
    "            cm[cid, idx] += 1\n",
    "        count[cid] += 1\n",
    "\n",
    "    for i in range(len(reviewed_classes)):\n",
    "        cm[i,:] /= count[i]\n",
    "    return cm\n",
    "\n",
    "cm_M = get_cm('M')\n",
    "cm_F = get_cm('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(30,10))\n",
    "sns.heatmap(cm_M, annot=True, xticklabels=reviewed_plus_classes+['other'], yticklabels=reviewed_classes, ax=axes[0], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "sns.heatmap(cm_F, annot=True, xticklabels=reviewed_plus_classes+['other'], yticklabels=reviewed_classes, ax=axes[1], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "axes[0].set_title('Male')\n",
    "axes[1].set_title('Female')\n",
    "\n",
    "plt.savefig('plots/cm_reviewed_label.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-wiring",
   "metadata": {},
   "source": [
    "### Label co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurence(gender):\n",
    "    cm = np.zeros((len(reviewed_plus_classes), len(reviewed_plus_classes)))\n",
    "\n",
    "    for sample in data_valid:\n",
    "        if not sample['gender'] == gender:\n",
    "            continue\n",
    "        new_ids = [reviewed_plus_classes.index(c) for c in sample['titles'] if c in reviewed_plus_classes]\n",
    "        for idx in new_ids:\n",
    "            for idx2 in new_ids:\n",
    "                cm[idx, idx2] += 1\n",
    "\n",
    "    for i in range(len(reviewed_plus_classes)):\n",
    "        cm[i,:] /= cm[i,i]\n",
    "\n",
    "    return cm\n",
    "\n",
    "cm_M = get_cooccurence('M')\n",
    "cm_F = get_cooccurence('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(30,10))\n",
    "sns.heatmap(cm_M, annot=True, xticklabels=reviewed_plus_classes, yticklabels=reviewed_plus_classes, ax=axes[0], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "sns.heatmap(cm_F, annot=True, xticklabels=reviewed_plus_classes, yticklabels=reviewed_plus_classes, ax=axes[1], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "axes[0].set_title('Male')\n",
    "axes[1].set_title('Female')\n",
    "\n",
    "plt.savefig('plots/label_cooccurence.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-perception",
   "metadata": {},
   "source": [
    "## Preparing the data for classification\n",
    "\n",
    "Please check for any stupid mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_one_label(data, classes, use_raw=True, use_review=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    gender = []\n",
    "    \n",
    "    for sample in data:\n",
    "        if use_review:\n",
    "            if not sample['valid']:\n",
    "                continue\n",
    "            labels = []\n",
    "            for title in sample['titles']:\n",
    "                if title in classes:\n",
    "                    labels.append(classes.index(title))\n",
    "            if not len(labels) == 1: # ignore any multi-class samples or those with no class of interest\n",
    "                continue\n",
    "                \n",
    "            if use_raw:\n",
    "                X.append(sample['raw'][sample['start_pos']:])\n",
    "            else:\n",
    "                X.append(sample['bio'])\n",
    "            y.append(labels[0])\n",
    "            gender.append(sample['gender'])\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if sample['auto_title'] not in classes:\n",
    "                continue\n",
    "            \n",
    "            if use_raw:\n",
    "                X.append(sample['raw'][sample['start_pos']:])\n",
    "            else:\n",
    "                X.append(sample['bio'])\n",
    "            y.append(classes.index(sample['auto_title']))\n",
    "            gender.append(sample['gender'])\n",
    "    \n",
    "    return X, y, gender\n",
    "\n",
    "# this one only applies for reviewed data\n",
    "def prepare_data_multi_label(data, classes, use_raw=True):\n",
    "    # TODO: if titles/raw titles not set derive them from the available raw titles and the title lookup\n",
    "    X = []\n",
    "    y = []\n",
    "    gender = []\n",
    "    \n",
    "    for sample in data:\n",
    "        if not sample['valid']:\n",
    "            continue\n",
    "        labels = []\n",
    "        for title in sample['titles']:\n",
    "            if title in classes:\n",
    "                labels.append(classes.index(title))\n",
    "        if len(labels) == 0: # there might be a few samples that don't fit into the reviewed classes anymore\n",
    "            continue\n",
    "\n",
    "        # create multi class label\n",
    "        lbl = np.array([1 if i in labels else 0 for i in range(len(classes))])\n",
    "\n",
    "        if use_raw:\n",
    "            X.append(sample['raw'][sample['start_pos']:])\n",
    "        else:\n",
    "            X.append(sample['bio'])\n",
    "        y.append(lbl)\n",
    "        gender.append(sample['gender'])\n",
    "    \n",
    "    return X, y, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAP(y_true, y_pred, group):\n",
    "    y_true_F = [y_true[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_true_M = [y_true[i] for i in range(len(group)) if group[i] == 'M']\n",
    "    y_pred_F = [y_pred[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_pred_M = [y_pred[i] for i in range(len(group)) if group[i] == 'M']\n",
    "\n",
    "    TP_F = [1 if y_true_F[i] == y_pred_F[i] else 0 for i in range(len(y_true_F))]\n",
    "    TP_F = sum(TP_F)/len(TP_F)\n",
    "\n",
    "    TP_M = [1 if y_true_M[i] == y_pred_M[i] else 0 for i in range(len(y_true_M))]\n",
    "    TP_M = sum(TP_M)/len(TP_M)\n",
    "\n",
    "    GAPS = TP_F - TP_M\n",
    "    return GAPS\n",
    "\n",
    "def GAP_binary(y_true, y_pred, group):\n",
    "    \n",
    "    y_true_F = [y_true[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_true_M = [y_true[i] for i in range(len(group)) if group[i] == 'M']\n",
    "    y_pred_F = [y_pred[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_pred_M = [y_pred[i] for i in range(len(group)) if group[i] == 'M']\n",
    "\n",
    "    TP_F = [1 if y_true_F[i] == 1 and y_pred_F[i] == 1 else 0 for i in range(len(y_true_F))]\n",
    "    TP_F = sum(TP_F)/len(TP_F)\n",
    "\n",
    "    TP_M = [1 if y_true_M[i] == 1 and y_pred_M[i] == 1 else 0 for i in range(len(y_true_M))]\n",
    "    TP_M = sum(TP_M)/len(TP_M)\n",
    "\n",
    "    GAPS = TP_F - TP_M\n",
    "    return GAPS\n",
    "\n",
    "def GAP_per_class(y_true, y_pred, group):\n",
    "    n_classes = max(y_true)+1\n",
    "    \n",
    "    y_true_F = [y_true[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_true_M = [y_true[i] for i in range(len(group)) if group[i] == 'M']\n",
    "    y_pred_F = [y_pred[i] for i in range(len(group)) if group[i] == 'F']\n",
    "    y_pred_M = [y_pred[i] for i in range(len(group)) if group[i] == 'M']\n",
    "\n",
    "    TP_F = np.zeros(n_classes)\n",
    "    sum_F = np.zeros(n_classes)\n",
    "    for i in range(len(y_true_F)):\n",
    "        if y_true_F[i] == y_pred_F[i]:\n",
    "            TP_F[y_true_F[i]] += 1\n",
    "        sum_F[y_true_F[i]] += 1\n",
    "    TP_F /= sum_F\n",
    "\n",
    "    TP_M = np.zeros(n_classes)\n",
    "    sum_M = np.zeros(n_classes)\n",
    "    for i in range(len(y_true_M)):\n",
    "        if y_true_M[i] == y_pred_M[i]:\n",
    "            TP_M[y_true_M[i]] += 1\n",
    "        sum_M[y_true_M[i]] += 1\n",
    "    TP_M /= sum_M\n",
    "    \n",
    "    GAPS = TP_F - TP_M\n",
    "    return GAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 5\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = len(reviewed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_ratio_per_class(y, gender, ref_gender='F'):\n",
    "    assert len(y) == len(gender)\n",
    "    \n",
    "    if type(y[0]) == np.ndarray: # one hot encoded label\n",
    "        return gender_ratio_multi_class(y, gender, ref_gender)\n",
    "    \n",
    "    # just one label per sample\n",
    "    ratios = []\n",
    "    for c in range(max(y)+1):\n",
    "        gender_c = [gender[i] for i in range(len(gender)) if y[i] == c]\n",
    "        \n",
    "        ratio = gender_c.count(ref_gender)/len(gender_c)\n",
    "        #print(c, ratio)\n",
    "        ratios.append(ratio)\n",
    "    return ratios\n",
    "\n",
    "def gender_ratio_multi_class(y, gender, ref_gender='F'):\n",
    "    ratios = []\n",
    "    for c in range(len(y[0])):\n",
    "        gender_c = [gender[i] for i in range(len(gender)) if y[i][c] == 1]\n",
    "        \n",
    "        ratio = gender_c.count(ref_gender)/len(gender_c)\n",
    "        #print(c, ratio)\n",
    "        ratios.append(ratio)\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-burning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_results(res):\n",
    "    (mean_f1s, std_f1s), (mean_accs, std_accs), (mean_gaps, std_gaps), (mean_gaps_per_class, std_gaps_per_class), (mean_gender_ratio, std_gender_ratio), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    print(\"F1: \", mean_f1s, \"+/-\", std_f1s)\n",
    "    print(\"Precision: \", mean_precision, \"+/-\", std_precision)\n",
    "    print(\"Recall: \", mean_recall, \"+/-\", std_recall)\n",
    "    print(\"Acc: \", mean_accs, \"+/-\", std_accs)\n",
    "\n",
    "    print(\"GAP: \", mean_gaps, \"+/-\", std_gaps)\n",
    "    print(\"GAP per class: \", mean_gaps_per_class, \"+/-\", std_gaps_per_class)\n",
    "\n",
    "def train_and_evaluate(init_model, X, y, gender, save_dir, multi_label=False):\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    accs = []\n",
    "    gaps = []\n",
    "    gaps_per_class = []\n",
    "    gender_ratios = []\n",
    "    cms_M = []\n",
    "    cms_F = []\n",
    "    \n",
    "    X, y, gender, = shuffle(X, y, gender)\n",
    "    \n",
    "    #set up n fold data    \n",
    "    n_folds = ITERATIONS\n",
    "    X_train_folds = np.array_split(X, n_folds)\n",
    "    y_train_folds = np.array_split(y, n_folds)\n",
    "    gender_train_folds = np.array_split(gender, n_folds)\n",
    "    \n",
    "    # check that any test set includes all class/gender combiatons\n",
    "    incomplete = False\n",
    "    for fold in range(n_folds):\n",
    "        y_test = y_train_folds[fold]\n",
    "        gender_test = gender_train_folds[fold]\n",
    "        incomplete_classes = []\n",
    "        if multi_label:\n",
    "            n_classes = len(y_test[0])\n",
    "            for c in range(n_classes):\n",
    "                gender_sel = [gender_test[i] for i in range(len(y_test)) if y_test[i][c] == 1]\n",
    "                if not 'M' in gender_sel or not 'F' in gender_sel:\n",
    "                    incomplete_classes.append(c)\n",
    "        else:\n",
    "            n_classes = max(y_test)+1\n",
    "            for c in range(n_classes):\n",
    "                gender_sel = [gender_test[i] for i in range(len(y_test)) if y_test[i] == c]\n",
    "                if not 'M' in gender_sel or not 'F' in gender_sel:\n",
    "                    incomplete_classes.append(c)\n",
    "                    \n",
    "        if len(incomplete_classes) > 0:\n",
    "            print(\"fold \", fold, \" does not contain M or F samples for classes:\", incomplete_classes)\n",
    "            incomplete = True\n",
    "            \n",
    "    if incomplete:\n",
    "        return\n",
    "        \n",
    "\n",
    "    for i in range(n_folds):\n",
    "        # set up filenames to save the data/model\n",
    "        model_dir = save_dir+str(i)+'/model/'\n",
    "        pred_file = save_dir+str(i)+'/predictions.pickle'\n",
    "        data_file = save_dir+str(i)+'/data.pickle'\n",
    "        \n",
    "        # get pretrained model\n",
    "        bert = init_model\n",
    "        \n",
    "        if not os.path.isdir(save_dir+str(i)):\n",
    "            os.makedirs(save_dir+str(i))\n",
    "        \n",
    "        if os.path.isfile(data_file):\n",
    "            with open(data_file, 'rb') as handle:\n",
    "                data_split = pickle.load(handle)\n",
    "                X_train = data_split['X_train']\n",
    "                X_test = data_split['X_test']\n",
    "                y_train = data_split['y_train']\n",
    "                y_test = data_split['y_test']\n",
    "                gender_train = data_split['gender_train']\n",
    "                gender_test = data_split['gender_test']\n",
    "                \n",
    "                # load existing predictions if possible\n",
    "                if os.path.isfile(pred_file):  \n",
    "                    print('Loading predictions pickle: ', pred_file)\n",
    "                    with open(pred_file, 'rb') as handle:\n",
    "                        y_pred = pickle.load(handle)\n",
    "                else:\n",
    "                    # train/ load model\n",
    "                    if os.path.isdir(model_dir):  \n",
    "                        print('Loading model from: ', model_dir)\n",
    "                        bert.load(model_dir)\n",
    "                    else:\n",
    "\n",
    "                        print('Retrain model...')\n",
    "                        bert.retrain(X_train, y_train, epochs=EPOCHS)\n",
    "                        print('Save model at: ', model_dir)\n",
    "                        bert.save(model_dir)\n",
    "                    \n",
    "                    # predict\n",
    "                    if multi_label:\n",
    "                        y_pred, y_idx = bert.predict(X_test, np.arange(len(y_test)))\n",
    "                        y_test = [y_test[i] for i in y_idx]\n",
    "                        gender_true = [gender_test[i] for i in y_idx]\n",
    "                        \n",
    "                        y_pred = np.array(y_pred) >= 0.5\n",
    "                \n",
    "                        # save y_test and gender_test according to shuffled test samples\n",
    "                        with open(data_file, 'wb') as handle:\n",
    "                            pickle.dump({'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'gender_train': gender_train, 'gender_test': gender_test}, handle)\n",
    "                    else:\n",
    "                        pred = bert.predict(X_test)\n",
    "                        y_pred = np.argmax(pred, axis=1)\n",
    "                        \n",
    "                    with open(pred_file, 'wb') as handle:\n",
    "                        pickle.dump(y_pred, handle)\n",
    "                        \n",
    "        else: # data split has not been saved\n",
    "            #X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(X, y, gender, test_size=0.33, random_state=i)\n",
    "            #set up training data\n",
    "            X_train = np.hstack(np.delete(X_train_folds,i, axis=0)).tolist()\n",
    "            if multi_label:\n",
    "                y_train = np.vstack(np.delete(y_train_folds,i, axis=0)).tolist()\n",
    "            else:\n",
    "                y_train = np.hstack(np.delete(y_train_folds,i, axis=0)).tolist()\n",
    "            gender_train = np.hstack(np.delete(gender_train_folds,i, axis=0)).tolist()\n",
    "            \n",
    "            #set up testing data\n",
    "            X_test = X_train_folds[i].tolist()\n",
    "            y_test = y_train_folds[i].tolist()\n",
    "            gender_test = gender_train_folds[i].tolist()\n",
    "            \n",
    "            with open(data_file, 'wb') as handle:\n",
    "                pickle.dump({'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'gender_train': gender_train, 'gender_test': gender_test}, handle)\n",
    "                \n",
    "            # if data wasn't saved, we need to train again\n",
    "            print('Retrain model...')\n",
    "            bert.retrain(X_train, y_train, epochs=EPOCHS)\n",
    "            print('Save model at: ', model_dir)\n",
    "            bert.save(model_dir)\n",
    "            \n",
    "            # and also predict again\n",
    "            if multi_label:\n",
    "                y_pred, y_idx = bert.predict(X_test, np.arange(len(y_test)))\n",
    "                y_test = [y_test[i] for i in y_idx]\n",
    "                gender_test = [gender_test[i] for i in y_idx]\n",
    "                \n",
    "                y_pred = np.array(y_pred) >= 0.5\n",
    "                \n",
    "                # save y_test and gender_test according to shuffled test samples\n",
    "                with open(data_file, 'wb') as handle:\n",
    "                    pickle.dump({'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'gender_train': gender_train, 'gender_test': gender_test}, handle)\n",
    "            else:\n",
    "                pred = bert.predict(X_test)\n",
    "                y_pred = np.argmax(pred, axis=1)\n",
    "            with open(pred_file, 'wb') as handle:\n",
    "                pickle.dump(y_pred, handle)\n",
    "                \n",
    "        if multi_label:\n",
    "            y_true = np.asarray(y_test)\n",
    "            \n",
    "            class_weights = (np.sum(y_test, axis=0)/np.sum(y_test))\n",
    "            f1s.append(np.sum([f1_score(y_true[:,c], y_pred[:,c], average='macro')*class_weights[c] for c in range(len(y_test[0]))]))\n",
    "            precs.append(np.sum([precision_score(y_true[:,c], y_pred[:,c], average='macro')*class_weights[c] for c in range(len(y_test[0]))]))\n",
    "            recs.append(np.sum([recall_score(y_true[:,c], y_pred[:,c], average='macro')*class_weights[c] for c in range(len(y_test[0]))]))\n",
    "            accs.append([accuracy_score(y_true[:,c], y_pred[:,c]) for c in range(len(y_test[0]))])\n",
    "            class_gaps = [GAP_binary(y_true[:,c], y_pred[:,c], gender_test) for c in range(len(y_test[0]))]\n",
    "            gaps_per_class.append(class_gaps)\n",
    "            gaps.append(np.mean(np.abs(class_gaps)))\n",
    "            \n",
    "        else:\n",
    "            # default scores for one-label classification\n",
    "            f1s.append(f1_score(y_test, y_pred, average='macro'))\n",
    "            precs.append(precision_score(y_test, y_pred, average='macro'))\n",
    "            recs.append(recall_score(y_test, y_pred, average='macro'))\n",
    "            accs.append(accuracy_score(y_test, y_pred))\n",
    "            class_gaps = GAP_per_class(y_test, y_pred, gender_test)\n",
    "            gaps.append(np.mean(np.abs(class_gaps)))\n",
    "            gaps_per_class.append(class_gaps)\n",
    "            # confusion matrices for M/F samples\n",
    "            cms_M.append(confusion_matrix([y for i, y in enumerate(y_test) if gender_test[i] == 'M'], [y for i, y in enumerate(y_pred) if gender_test[i] == 'M']))\n",
    "            cms_F.append(confusion_matrix([y for i, y in enumerate(y_test) if gender_test[i] == 'F'], [y for i, y in enumerate(y_pred) if gender_test[i] == 'F']))\n",
    "        \n",
    "        if multi_label:\n",
    "            gender_ratios.append(gender_ratio_multi_class(y_train, gender_train))\n",
    "        else:\n",
    "            gender_ratios.append(gender_ratio_per_class(y_train, gender_train))\n",
    "    \n",
    "    res = (np.mean(f1s), np.std(f1s)), (np.mean(accs), np.std(accs)), (np.mean(gaps), np.std(gaps)), (np.mean(gaps_per_class, axis=0), np.std(gaps_per_class, axis=0)), (np.mean(gender_ratios, axis=0), np.std(gender_ratios, axis=0)), (np.mean(precs, axis=0), np.std(precs, axis=0)), (np.mean(recs, axis=0), np.std(recs, axis=0))\n",
    "    print_results(res)\n",
    "    \n",
    "    return res, cms_M, cms_F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cms_M, cms_F, classes, filename):\n",
    "    mean_cm_M = np.mean(cms_M, axis=0)\n",
    "    mean_cm_F = np.mean(cms_F, axis=0)\n",
    "    for i in range(len(classes)):\n",
    "        mean_cm_M[i,:] /= sum(mean_cm_M[i,:])\n",
    "        mean_cm_F[i,:] /= sum(mean_cm_F[i,:])\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(30,10))\n",
    "    sns.heatmap(mean_cm_M, annot=True, xticklabels=classes, yticklabels=classes, ax=axes[0], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "    sns.heatmap(mean_cm_F, annot=True, xticklabels=classes, yticklabels=classes, ax=axes[1], fmt=\".2f\", annot_kws={'fontsize': 15})\n",
    "    axes[0].set_title('Male')\n",
    "    axes[1].set_title('Female')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-budget",
   "metadata": {},
   "source": [
    "## Unsupervised data (raw texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, gender = prepare_data_one_label(data_reviewed, reviewed_classes, use_raw=True, use_review=False)\n",
    "print(len(X))\n",
    "\n",
    "bert = BertHuggingface(NUM_CLASSES, batch_size=BATCH_SIZE)\n",
    "results['unsupervised-raw'], cms_M, cms_F = train_and_evaluate(bert, X, y, gender, 'results/unsupervised_raw/', multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cms_M, cms_F, reviewed_classes, 'plots/cm_UR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-hygiene",
   "metadata": {},
   "source": [
    "F1:  0.7825470346290064 +/- 0.0\n",
    "\n",
    "Acc:  0.7929125138427464 +/- 0.0\n",
    "\n",
    "GAP:  0.04386221480895913 +/- 0.0\n",
    "\n",
    "GAP per class:  0.02529047354879495 +/- 0.11668768674783181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-photography",
   "metadata": {},
   "source": [
    "## Unsupervised data (gender-scrubbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, gender = prepare_data_one_label(data_reviewed, reviewed_classes, use_raw=False, use_review=False)\n",
    "print(len(X))\n",
    "\n",
    "bert = BertHuggingface(NUM_CLASSES, batch_size=BATCH_SIZE)\n",
    "results['unsupervised-scrubbed'], cms_M, cms_F = train_and_evaluate(bert, X, y, gender, 'results/unsupervised_scrubbed/', multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cms_M, cms_F, reviewed_classes, 'plots/cm_US.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-chance",
   "metadata": {},
   "source": [
    "## Reviewed data (one label, raw texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_plus_classes = reviewed_classes + ['*software architect']\n",
    "X, y, gender = prepare_data_one_label(data_reviewed, reviewed_plus_classes, use_raw=True, use_review=True)\n",
    "print(len(X))\n",
    "\n",
    "bert = BertHuggingface(len(reviewed_plus_classes), batch_size=BATCH_SIZE)\n",
    "results['reviewed-single-raw'], cms_M, cms_F = train_and_evaluate(bert, X, y, gender, 'results/reviewed_single_raw/', multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cms_M, cms_F, reviewed_plus_classes, 'plots/cm_RR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-street",
   "metadata": {},
   "source": [
    "F1:  0.8544357417766737 +/- 0.0\n",
    "    \n",
    "Acc:  0.8741652021089632 +/- 1.1102230246251565e-16\n",
    "    \n",
    "GAP:  0.03245962385229917 +/- 0.0\n",
    "    \n",
    "GAP per class:  -0.018746219284855353 +/- 0.1109400479883512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-light",
   "metadata": {},
   "source": [
    "## Reviewed data (one label, gender-scrubbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_plus_classes = reviewed_classes + ['*software architect']\n",
    "X, y, gender = prepare_data_one_label(data_reviewed, reviewed_plus_classes, use_raw=False, use_review=True)\n",
    "print(len(X))\n",
    "\n",
    "bert = BertHuggingface(len(reviewed_plus_classes), batch_size=BATCH_SIZE)\n",
    "results['reviewed-single-scrubbed'], cms_M, cms_F = train_and_evaluate(bert, X, y, gender, 'results/reviewed_single_scrubbed/', multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cms_M, cms_F, reviewed_plus_classes, 'plots/cm_RS.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-automation",
   "metadata": {},
   "source": [
    "F1:  0.8581141473983337 +/- 1.1102230246251565e-16\n",
    "\n",
    "Acc:  0.8773286467486819 +/- 0.0\n",
    "\n",
    "GAP:  0.03647492855708645 +/- 0.0\n",
    "\n",
    "GAP per class:  0.00790687262458947 +/- 0.09756611854625045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-presentation",
   "metadata": {},
   "source": [
    "## BERT for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.input_ids)\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_classes, batch_size=8, bert_model='bert-base-uncased'):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
    "        self.model = transformers.BertModel.from_pretrained(bert_model)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, num_classes)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(params = self.model.parameters(), lr=1e-5)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.model(input_ids, attention_mask = attention_mask)\n",
    "        #print(output_1)\n",
    "        pooled_output = output_1[1]\n",
    "        output_2 = self.l2(pooled_output)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "    def save(self, save_dir):\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        file = save_dir+\"multi_bert.pickle\"\n",
    "        with open(file, 'wb') as handle:\n",
    "            pickle.dump({'model': self.model, 'l2': self.l2, 'l3': self.l3}, handle)\n",
    "            \n",
    "    def load(self, save_dir):\n",
    "        file = save_dir+\"multi_bert.pickle\"\n",
    "        with open(file, 'rb') as handle:\n",
    "            save_data = pickle.load(handle)\n",
    "            self.model = save_data['model']\n",
    "            self.l2 = save_data['l2']\n",
    "            self.l3 = save_data['l3']\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.model.to(device)\n",
    "        self.l2.to(device)\n",
    "        self.l3.to(device)\n",
    "    \n",
    "    def predict(self, texts, idx):\n",
    "        y_pred = []\n",
    "        indeces = []\n",
    "        \n",
    "        # prepare dataset\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "        inputs['sample_ids'] = torch.tensor(idx)\n",
    "        dataset = BertDataset(inputs)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        self.to(device)\n",
    "        \n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = self.forward(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            outputs = outputs.to('cpu')\n",
    "            outputs = outputs.detach().numpy()\n",
    "            y_pred.append(outputs)\n",
    "            indeces.append(batch['sample_ids'])\n",
    "\n",
    "            input_ids = input_ids.to('cpu')\n",
    "            attention_mask = attention_mask.to('cpu')\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "                \n",
    "        y_pred = np.vstack(y_pred)\n",
    "        indeces = np.hstack(indeces)\n",
    "        \n",
    "        return y_pred, indeces\n",
    "        \n",
    "    def retrain(self, texts, labels, epochs):\n",
    "        # prepare dataset\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "        inputs['labels'] = torch.tensor(labels)\n",
    "        dataset = BertDataset(inputs)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        self.model.train()\n",
    "        self.to(device)\n",
    "        optimizer = transformers.AdamW(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loop = tqdm(loader, leave=True)\n",
    "            for batch in loop:\n",
    "                # initialize calculated gradients (from prev step)\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # process\n",
    "                outputs = self.forward(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                # extract loss\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                outputs.to('cpu')\n",
    "\n",
    "                # calculate loss for every parameter that needs grad update\n",
    "                loss.backward()\n",
    "\n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "\n",
    "                # print relevant info to progress bar\n",
    "                loop.set_description(f'Epoch {epoch}')\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "\n",
    "                loss = loss.detach().item()\n",
    "                \n",
    "                input_ids = input_ids.to('cpu')\n",
    "                attention_mask = attention_mask.to('cpu')\n",
    "                labels = labels.to('cpu')\n",
    "                del input_ids\n",
    "                del attention_mask\n",
    "                del labels\n",
    "\n",
    "        self.model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "def loss_fn(outputs, targets):\n",
    "    outputs = outputs.float()\n",
    "    targets = targets.float()\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-steal",
   "metadata": {},
   "source": [
    "## Reviewed data (multi-label, raw texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_plus_classes = reviewed_classes + ['*software architect', '*writer', '*researcher']\n",
    "X, y, gender = prepare_data_multi_label(data_reviewed, reviewed_plus_classes, use_raw=True)\n",
    "print(len(X))\n",
    "\n",
    "bert = BERTClass(len(reviewed_plus_classes), batch_size=BATCH_SIZE)\n",
    "results['reviewed-multi-raw'], _, _ = train_and_evaluate(bert, X, y, gender, 'results/reviewed_multi_raw/', multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-antibody",
   "metadata": {},
   "source": [
    "## Reviewed data (multi-label, gender-scrubbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_plus_classes = reviewed_classes + ['*software architect', '*writer', '*researcher']\n",
    "X, y, gender = prepare_data_multi_label(data_reviewed, reviewed_plus_classes, use_raw=False)\n",
    "print(len(X))\n",
    "\n",
    "bert = BERTClass(len(reviewed_plus_classes), batch_size=BATCH_SIZE)\n",
    "results['reviewed-multi-scrubbed'], _, _ = train_and_evaluate(bert, X, y, gender, 'results/reviewed_multi_scrubbed/', multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-rabbit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "connected-fruit",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "f1_err = []\n",
    "prec = []\n",
    "prec_err = []\n",
    "rec = []\n",
    "rec_err = []\n",
    "acc = []\n",
    "acc_err = []\n",
    "gap = []\n",
    "gap_err = []\n",
    "for key, res in results.items():\n",
    "    (mean_f1s, std_f1s), (mean_accs, std_accs), (mean_gaps, std_gaps), (mean_gaps_per_class, std_gaps_per_class), (gender_ratio, _), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    f1.append(mean_f1s)\n",
    "    f1_err.append(std_f1s)\n",
    "    prec.append(mean_precision)\n",
    "    prec_err.append(std_precision)\n",
    "    rec.append(mean_recall)\n",
    "    rec_err.append(std_recall)\n",
    "    acc.append(mean_accs)\n",
    "    acc_err.append(std_accs)\n",
    "    gap.append(mean_gaps)\n",
    "    gap_err.append(std_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(f1))\n",
    "width = 0.25\n",
    "offset = width/2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects2 = ax.bar(x - offset, acc, width, yerr=acc_err, label=\"Accuracy\", color='tab:blue')\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "keys = ['unsupervised \\n(w)', 'unsupervised \\n(w/o)', 'reviewed-\\nsingle (w)', 'reviewed-\\nsingle (w/o)', 'reviewed-\\nmulti (w)', 'reviewed-\\nmulti (w/o)']\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(keys, rotation=45)\n",
    "ax.set_ylabel('Accuracy', color='tab:blue')\n",
    "ax.set_title('Accuracy and GAP per dataset versions')\n",
    "\n",
    "rects3 = ax2.bar(x + offset, gap, width, yerr=gap_err, label=\"GAP\", color='tab:green')\n",
    "ax2.set_ylabel('mean absolute GAP', color='tab:green')\n",
    "\n",
    "ax.set_ylim(0.0, 1.19)\n",
    "\n",
    "plt.grid(False)\n",
    "\n",
    "plt.savefig('plots/acc_gap.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(f1))\n",
    "width = 0.25\n",
    "offset = width/2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "rects = ax.bar(x - offset, f1, width, yerr=f1_err, label=\"F1\")\n",
    "rects2 = ax.bar(x + offset, prec, width, yerr=acc_err, label=\"Precison\")\n",
    "rects3 = ax.bar(x + 3*offset, rec, width, yerr=gap_err, label=\"Recall\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.keys(), rotation=45)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Performance per dataset versions')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_ylim(0.0, 1.19)\n",
    "\n",
    "plt.savefig('plots/f1_prec_rec.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise GAPS\n",
    "\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "n_classes = 13\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "plt.grid(True)\n",
    "for key, res in results.items(): #TODO one plot for each dataset (unsupervised/one-class/multi-class or raw/scrubbed)\n",
    "    if 'scrubbed' in key:\n",
    "        continue\n",
    "    (_, _), (_, _), (_, _), (mean_gaps_per_class, std_gaps_per_class), (gender_ratio, _), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    measurement = mean_gaps_per_class[:n_classes]\n",
    "    print(measurement)\n",
    "    errors = std_gaps_per_class[:n_classes]\n",
    "    x = np.arange(len(measurement))\n",
    "    offset = width * multiplier\n",
    "    lbl = key.replace('-raw', ' (w)')\n",
    "    rects = ax.bar(x + offset, measurement, width, yerr=errors, label=lbl)\n",
    "    multiplier += 1\n",
    "    \n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('GAPs')\n",
    "ax.set_title('Class-wise GAP scores')\n",
    "ax.set_xticks(np.arange(len(reviewed_plus_classes)) + width)\n",
    "ax.set_xticklabels(reviewed_plus_classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('plots/class_gaps.eps', format='eps', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise GAPS\n",
    "\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "n_classes = 13\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "for key, res in results.items(): #TODO one plot for each dataset (unsupervised/one-class/multi-class or raw/scrubbed)\n",
    "    if 'raw' in key:\n",
    "        continue\n",
    "    (_, _), (_, _), (_, _), (mean_gaps_per_class, std_gaps_per_class), (gender_ratio, _), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    measurement = mean_gaps_per_class[:n_classes]\n",
    "    print(measurement)\n",
    "    errors = std_gaps_per_class[:n_classes]\n",
    "    x = np.arange(len(measurement))\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, yerr=errors, label=key)\n",
    "    multiplier += 1\n",
    "    \n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('GAPs')\n",
    "ax.set_title('Class-wise GAP scores')\n",
    "ax.set_xticks(np.arange(len(reviewed_plus_classes)) + width)\n",
    "ax.set_xticklabels(reviewed_plus_classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.grid()\n",
    "plt.savefig('plots/class_gaps.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise GAPS\n",
    "\n",
    "width = 0.2  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "n_classes = 13\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "for key, res in results.items(): #TODO one plot for each dataset (unsupervised/one-class/multi-class or raw/scrubbed)\n",
    "    if 'single' in key:\n",
    "        continue\n",
    "    (_, _), (_, _), (_, _), (mean_gaps_per_class, std_gaps_per_class), (gender_ratio, _), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    measurement = mean_gaps_per_class[:n_classes]\n",
    "    print(measurement)\n",
    "    errors = std_gaps_per_class[:n_classes]\n",
    "    x = np.arange(len(measurement))\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, yerr=errors, label=key)\n",
    "    #ax.bar_label(rects, padding=3, fmt='%.2f', fontsize=15)\n",
    "    multiplier += 1\n",
    "    \n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('GAPs')\n",
    "ax.set_title('Class-wise GAP scores')\n",
    "ax.set_xticks(np.arange(len(reviewed_plus_classes)) + width)\n",
    "ax.set_xticklabels(reviewed_plus_classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('plots/class_gaps.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class-wise GAPS\n",
    "\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "n_classes = 13\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "for key, res in results.items(): #TODO one plot for each dataset (unsupervised/one-class/multi-class or raw/scrubbed)\n",
    "    if 'scrubbed' in key:\n",
    "        continue\n",
    "    (_, _), (_, _), (_, _), (_, _), (mean_gender_ratio, std_gender_ratio), (mean_precision, std_precision), (mean_recall, std_recall) = res\n",
    "    measurement = mean_gender_ratio[:n_classes]\n",
    "    print(measurement)\n",
    "    errors = std_gender_ratio[:n_classes]\n",
    "    x = np.arange(len(measurement))\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, yerr=errors, label=key)\n",
    "    multiplier += 1\n",
    "    \n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Female Ratio')\n",
    "ax.set_title('Female Ratio per Occupation')\n",
    "ax.set_xticks(np.arange(len(reviewed_plus_classes)) + width)\n",
    "ax.set_xticklabels(reviewed_plus_classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('plots/female_ratio.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'green']\n",
    "#fig, axes = plt.subplots(1,3, figsize=(24,8))\n",
    "gaps = []\n",
    "ratios = []\n",
    "hues = []\n",
    "keys = []\n",
    "for i, key in enumerate(['unsupervised-', 'reviewed-single-', 'reviewed-multi-']):\n",
    "    for j, version in enumerate(['raw', 'scrubbed']):\n",
    "        (_, _), (_, _), (_, _), (mean_gaps_per_class, std_gaps_per_class), (mean_gender_ratio, std_gender_ratio), (mean_precision, std_precision), (mean_recall, std_recall) = results[key+version]\n",
    "        \n",
    "        gaps.append(mean_gaps_per_class)\n",
    "        ratios.append(mean_gender_ratio)\n",
    "        if version == 'raw':\n",
    "            hues += ['w']*len(mean_gaps_per_class)\n",
    "        else:\n",
    "            hues += ['w/o']*len(mean_gaps_per_class)\n",
    "        keys += [key[:-1]]*len(mean_gaps_per_class)\n",
    "        \n",
    "        print(key+version)\n",
    "        print(\"R: \", pearsonr(mean_gender_ratio, mean_gaps_per_class)) \n",
    "        \n",
    "gaps = np.hstack(gaps)\n",
    "ratios = np.hstack(ratios)\n",
    "df = pd.DataFrame(data={'GAP (female)': gaps, 'female ratio': ratios, 'gender ind.': hues, 'dataset': keys})\n",
    "\n",
    "g = sns.FacetGrid(df, col=\"dataset\", hue='gender ind.', palette=colors, height=6, legend_out=False)\n",
    "g.map_dataframe(sns.regplot, x='female ratio', y='GAP (female)')\n",
    "g.add_legend()\n",
    "\n",
    "plt.savefig('plots/correlation.png', bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-cinema",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-attempt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
