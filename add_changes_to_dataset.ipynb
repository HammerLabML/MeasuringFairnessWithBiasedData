{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "painted-electricity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "durable-vegetable",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIOS_FILE_REVIEWED = \"data/BIOS_REVIEWED.pkl\" # only reviewed samples\n",
    "BIOS_FILE_COMPLETE = \"data/BIOS_COMPLETE.pkl\" # whole dataset with changes to the reviewed samples\n",
    "BIOS_LABELS = \"data/BIOS_LABELS.csv\" # this contains all of our changes without the sensitive data, but ids matching the BIOS dataset\n",
    "BIOS_RAW = \"../../data/biosbias2023/BIOS.pkl\" # \"data/BIOS.pkl\"\n",
    "TITLE_JSON = \"data/title_lookup.json\"\n",
    "\n",
    "reviewed_classes = ['architect', 'surgeon', 'dentist', 'teacher', 'psychologist', 'nurse', 'photographer', 'physician', 'attorney', 'journalist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3c44fd-d0db-4368-9eab-a950ae190832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with (open(BIOS_RAW, \"rb\")) as openfile:\n",
    "    raw_data = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beneficial-columbia",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55734/1930653701.py:1: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_label = pd.read_csv(BIOS_LABELS, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "df_label = pd.read_csv(BIOS_LABELS, sep='\\t')\n",
    "labeled_data = df_label.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fatal-hybrid",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(labeled_data)):\n",
    "    if not (raw_data[i]['raw_title'] == labeled_data[i]['auto_raw_title'] and raw_data[i]['start_pos'] == labeled_data[i]['start_pos']):\n",
    "        print(\"data mismatch at id \", i)\n",
    "        \n",
    "    labeled_data[i]['raw'] = raw_data[i]['raw']\n",
    "    labeled_data[i]['bio'] = raw_data[i]['bio']\n",
    "    labeled_data[i]['name'] = raw_data[i]['name']\n",
    "    \n",
    "    # convert titles from nan/string to list\n",
    "    if type(labeled_data[i]['titles']) == float:\n",
    "        labeled_data[i]['titles'] = []\n",
    "    elif type(labeled_data[i]['titles']) == str:\n",
    "        titles = labeled_data[i]['titles'].strip(']\"\\'[').split('\\', \\'')\n",
    "        labeled_data[i]['titles'] = titles\n",
    "        \n",
    "    # convert raw titles fron nan/string to list\n",
    "    if type(labeled_data[i]['raw_titles']) == float:\n",
    "        labeled_data[i]['raw_titles'] = []\n",
    "    elif type(labeled_data[i]['titles']) == str:\n",
    "        titles = labeled_data[i]['raw_titles'].strip(']\"\\'[').split(', ')\n",
    "        labeled_data[i]['raw_titles'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desirable-dealing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=labeled_data)\n",
    "df_reviewed = df[df['review'] == 1]\n",
    "reviewed_data = df_reviewed.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "independent-timeline",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with (open(BIOS_FILE_COMPLETE, \"wb\")) as openfile:\n",
    "    pickle.dump(labeled_data, openfile)\n",
    "\n",
    "with (open(BIOS_FILE_REVIEWED, \"wb\")) as openfile:\n",
    "    pickle.dump(reviewed_data, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7353a9-03ec-4c62-9028-828b9c3a0221",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Update: Prepare the reviewed data for merging with huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d931712d-f745-42c9-bb64-91ddf56c1ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/saschroeder/.cache/huggingface/datasets/LabHC___parquet/LabHC--bias_in_bios-0590f29daf9e7342/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e90918079984974b09572ebea00d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'hard_text': Value(dtype='string', id=None), 'profession': Value(dtype='int64', id=None), 'gender': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits={'train': SplitInfo(name='train', num_bytes=107552255, num_examples=257478, shard_lengths=None, dataset_name='parquet'), 'test': SplitInfo(name='test', num_bytes=41337024, num_examples=99069, shard_lengths=None, dataset_name='parquet'), 'dev': SplitInfo(name='dev', num_bytes=16514329, num_examples=39642, shard_lengths=None, dataset_name='parquet')}, download_checksums={'https://huggingface.co/datasets/LabHC/bias_in_bios/resolve/052f01de644dba841176e0449528b41f27d94a61/data/train-00000-of-00001-0ab65b32c47407e8.parquet': {'num_bytes': 64908645, 'checksum': None}, 'https://huggingface.co/datasets/LabHC/bias_in_bios/resolve/052f01de644dba841176e0449528b41f27d94a61/data/test-00000-of-00001-5598c840ce8de1ee.parquet': {'num_bytes': 24947147, 'checksum': None}, 'https://huggingface.co/datasets/LabHC/bias_in_bios/resolve/052f01de644dba841176e0449528b41f27d94a61/data/dev-00000-of-00001-e6551072fff26949.parquet': {'num_bytes': 9952546, 'checksum': None}}, download_size=99808338, post_processing_size=None, dataset_size=165403608, size_in_bytes=265211946)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "\n",
    "# labels in the order specified here: https://huggingface.co/datasets/LabHC/bias_in_bios\n",
    "# not included in the datasets metadata\n",
    "huggingface_label = ['accountant', 'architect', 'attorney', 'chiropractor', 'comedian', 'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', 'interior_designer', 'journalist', 'model', 'nurse', 'painter', 'paralegal', 'pastor', 'personal_trainer', 'photographer', 'physician', 'poet', 'professor', 'psychologist', 'rapper', 'software_engineer', 'surgeon', 'teacher', 'yoga_teacher']\n",
    "partitions = ['train', 'test', 'dev']\n",
    "\n",
    "dataset['train'].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247cfae8-f7c7-4847-bee6-b6b5a5adba17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in labeled_data:\n",
    "    sample['match_text'] = sample['raw'][sample['start_pos']:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537f2c41-dad4-4393-9ce7-4ab4a4c40bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load precomputed matches\n"
     ]
    }
   ],
   "source": [
    "MATCH_PKL = 'matches.pkl'\n",
    "if not os.path.isfile(MATCH_PKL):\n",
    "    matches = {part: [] for part in partitions}\n",
    "    reverse_matches = [[] for sample in labeled_data]\n",
    "\n",
    "    for part in partitions:\n",
    "        matches[part] = []\n",
    "        print(part)\n",
    "        for i in tqdm(range(len(dataset[part]))):\n",
    "            text = dataset[part][i]['hard_text']\n",
    "            f = list(filter(lambda labeled_data: labeled_data['match_text'] == text, labeled_data))\n",
    "            potential_matches = []\n",
    "            for elem in f:\n",
    "                if elem['auto_title'] == huggingface_label[dataset[part][i]['profession']]:\n",
    "                    potential_matches.append(elem['Unnamed: 0'])\n",
    "                    reverse_matches[elem['Unnamed: 0']].append((part,i))\n",
    "            matches[part].append((i,potential_matches))\n",
    "\n",
    "    with open(MATCH_PKL, 'wb') as handle:\n",
    "        pickle.dump({'matches': matches, 'reverse_matches': reverse_matches}, handle)\n",
    "    \n",
    "else:\n",
    "    print(\"load precomputed matches\")\n",
    "    with open(MATCH_PKL, 'rb') as handle:\n",
    "        loaded_matches = pickle.load(handle)\n",
    "        matches = loaded_matches['matches']\n",
    "        reverse_matches = loaded_matches['reverse_matches']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac38f56c-11f4-4a6c-8259-7ff270277e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "got 257478 samples in total\n",
      "for 1197 samples found no match in the labeled data\n",
      "for 933 samples found multiple matches in the labeled data\n",
      "test\n",
      "got 99069 samples in total\n",
      "for 456 samples found no match in the labeled data\n",
      "for 343 samples found multiple matches in the labeled data\n",
      "dev\n",
      "got 39642 samples in total\n",
      "for 171 samples found no match in the labeled data\n",
      "for 145 samples found multiple matches in the labeled data\n",
      "based on the crawled dataset\n",
      "got 397907 samples in total\n",
      "for 3530 samples found no match in the labeled data\n",
      "for 1412 samples found multiple matches in the labeled data\n"
     ]
    }
   ],
   "source": [
    "for part in partitions:\n",
    "    print(part)\n",
    "    count0 = 0\n",
    "    count_double = 0\n",
    "\n",
    "    for match in matches[part]:\n",
    "        if match[1] == []:\n",
    "            count0 += 1\n",
    "        elif len(match[1]) > 1:\n",
    "            count_double += 1\n",
    "\n",
    "    print(\"got %i samples in total\" % len(matches[part]))\n",
    "    print(\"for %i samples found no match in the labeled data\" % count0)\n",
    "    print(\"for %i samples found multiple matches in the labeled data\" % count_double)\n",
    "    \n",
    "\n",
    "count0 = 0\n",
    "count_multi = 0\n",
    "for match in reverse_matches:\n",
    "    n_matches = len(match)\n",
    "    if n_matches == 0:\n",
    "        count0 += 1\n",
    "    if n_matches > 1:\n",
    "        count_multi += 1\n",
    "        \n",
    "print(\"based on the crawled dataset\")\n",
    "print(\"got %i samples in total\" % len(reverse_matches))\n",
    "print(\"for %i samples found no match in the labeled data\" % count0)\n",
    "print(\"for %i samples found multiple matches in the labeled data\" % count_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59dd0884-75f2-47f1-89d1-a382f78ed679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_text': 'He is also the project lead of and major contributor to the open source assembler/simulator \"EASy68K.\" He earned a master’s degree in computer science from the University of Michigan-Dearborn, where he is also an adjunct instructor. Downloads/Updates',\n",
       " 'profession': 21,\n",
       " 'gender': 0,\n",
       " 'split': 'train',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_filter = []\n",
    "for part in partitions:\n",
    "    for i, sample in enumerate(dataset[part]):\n",
    "        sample_for_filter = sample.copy()\n",
    "        sample_for_filter.update({'split': part, 'id': i})\n",
    "        dataset_for_filter.append(sample_for_filter)\n",
    "        \n",
    "dataset_for_filter[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "703f8d86-f10b-4b34-bd10-5c6c09114176",
   "metadata": {
    "tags": []
   },
   "source": [
    "text = dataset['train'][242]['hard_text']\n",
    "f = list(filter(lambda dataset_for_filter: dataset_for_filter['hard_text'] == text, dataset_for_filter))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe31caa-14ee-42be-813b-cbad29eb7d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from unidecode import unidecode\n",
    "#unidecode('kožušček')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92701f3e-7815-4212-b701-c9dd277bab7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "def print_sample(sample):\n",
    "    for k in ['name', 'gender', 'auto_title', 'titles', 'review', 'valid', 'raw']:\n",
    "        print(k+\": \", sample[k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36daf615-b6ca-4cdf-9cee-6f9c97f66595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 257478/257478 [3:04:58<00:00, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 99069/99069 [1:10:43<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 39642/39642 [28:18<00:00, 23.34it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_dataset = {part: [] for part in partitions}\n",
    "dup_id_list = []\n",
    "\n",
    "for part in partitions:\n",
    "    print(part)\n",
    "    for match in tqdm(matches[part]):\n",
    "        idx = match[0]\n",
    "        sample = dataset[part][idx]\n",
    "        text = sample['hard_text']\n",
    "\n",
    "        # duplicates in the huggingface dataset:\n",
    "        f = list(filter(lambda dataset_for_filter: dataset_for_filter['hard_text'] == text, dataset_for_filter))\n",
    "        sample.update({'text_duplicates': [(elem['split'], elem['id']) for elem in f if (elem['id'] != idx or elem['split'] != part)]})\n",
    "\n",
    "        if len(match[1]) == 0:\n",
    "            # keep old label\n",
    "            sample.update({'titles_supervised': [], 'raw_titles_supervised': [], 'gender_supervised': '', 'auto_title': '', 'auto_raw_title': '', 'path': '', 'URI': '', \n",
    "                           'review': 0, 'valid': -1, 'style_valid': -1, 'raw': '', 'bio': '', 'name': ('',''), 'duplicates': []})\n",
    "            merged_dataset[part].append(sample)\n",
    "        elif len(match[1]) > 1:\n",
    "            dup_id_list.append(match[0])\n",
    "            duplicates = []\n",
    "            for m_id in match[1]:\n",
    "                for d_tup in reverse_matches[m_id]:\n",
    "                    if d_tup[1] != idx and d_tup not in duplicates:\n",
    "                        duplicates.append(d_tup)\n",
    "\n",
    "            same_number_of_duplicates = (len(duplicates)+1 == len(match[1]))\n",
    "            all_equal = True\n",
    "            if not same_number_of_duplicates:\n",
    "                base_sample = labeled_data[match[1][0]]\n",
    "                #if len(duplicates) > 0 and len(match[1]) > 1:\n",
    "                #    print(\"inconsistent number of duplicates: %i vs. %i\" % (len(duplicates)+1, len(match[1])))\n",
    "                #    print(idx, match[1][0], \"base sample: \")\n",
    "                #    print_sample(base_sample)\n",
    "                for next_id in match[1][1:]:\n",
    "                    comp_sample = labeled_data[next_id]\n",
    "                    if not (comp_sample['titles'] == base_sample['titles'] and comp_sample['gender'] == base_sample['gender']):\n",
    "                        all_equal = False\n",
    "                assert all_equal, \"inconsistent number of duplicates in both dataset versions\"+str(match[1])+ \" vs. \" + str(duplicates)+\" for idx \"+str(idx)\n",
    "\n",
    "                # take first possible match\n",
    "                match_id = match[1][0]\n",
    "                merge_sample = labeled_data[match_id]\n",
    "\n",
    "                # assign other match ids to duplicates (1:1), handle inconsistent numbers of duplicates!\n",
    "                for i, (d_split,d_idx) in enumerate(duplicates):\n",
    "                    if i+1 >= len(matches[d_split][d_idx][1]):\n",
    "                        i -= len(matches[d_split][d_idx][1])\n",
    "                    matches[d_split][d_idx] = (matches[d_split][d_idx][0], [matches[d_split][d_idx][1][i+1]])\n",
    "                    #print(d_split, d_idx, matches[d_split][d_idx][1], \"->\", [matches[d_split][d_idx][1][i]])\n",
    "            else:\n",
    "                # take first possible match\n",
    "                match_id = match[1][0]\n",
    "                merge_sample = labeled_data[match_id]\n",
    "\n",
    "                # assign other match ids to duplicates (1:1)\n",
    "                for i, (d_split,d_idx) in enumerate(duplicates):\n",
    "                    matches[d_split][d_idx] = (matches[d_split][d_idx][0], [matches[d_split][d_idx][1][i+1]])\n",
    "                    #print(matches[d_split][d_idx][1], \"->\", [matches[d_split][d_idx][1][i]])\n",
    "\n",
    "            sample.update({'titles_supervised': merge_sample['titles'], 'raw_titles_supervised': merge_sample['raw_titles'], 'gender_supervised': merge_sample['gender'], \n",
    "                           'auto_title': merge_sample['auto_title'], 'auto_raw_title': merge_sample['auto_raw_title'],  'path': merge_sample['path'], 'URI': merge_sample['URI'], \n",
    "                           'review': merge_sample['review'], 'valid': merge_sample['valid'], 'style_valid': merge_sample['style_valid'], 'raw': merge_sample['raw'], \n",
    "                           'bio': merge_sample['bio'], 'name': merge_sample['name'], 'duplicates': duplicates})\n",
    "            merged_dataset[part].append(sample)\n",
    "\n",
    "        else:\n",
    "            match_id = match[1][0]\n",
    "            merge_sample = labeled_data[match_id]\n",
    "            sample.update({'titles_supervised': merge_sample['titles'], 'raw_titles_supervised': merge_sample['raw_titles'], 'gender_supervised': merge_sample['gender'], \n",
    "                           'auto_title': merge_sample['auto_title'], 'auto_raw_title': merge_sample['auto_raw_title'],  'path': merge_sample['path'], 'URI': merge_sample['URI'], \n",
    "                           'review': merge_sample['review'], 'valid': merge_sample['valid'], 'style_valid': merge_sample['style_valid'], 'raw': merge_sample['raw'], \n",
    "                           'bio': merge_sample['bio'], 'name': merge_sample['name'], 'duplicates': []})\n",
    "            merged_dataset[part].append(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52727fdc-45a0-4e58-9166-fe68e63c19d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(merged_dataset[part]):\n",
    "    if len(sample['duplicates']) > 0:\n",
    "        for (split, idx) in sample['duplicates']:\n",
    "            if len(merged_dataset[split][idx]['duplicates']) == 0:\n",
    "                dup = [tup for tup in sample['duplicates'] if not tup == (split,idx)]\n",
    "                dup.append((part, i))\n",
    "                merged_dataset[split][idx]['duplicates'] = dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa2a00a-152d-4094-b45a-2b60cc333ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/huggingface_merge_complete.pkl', 'wb') as handle:\n",
    "    pickle.dump(merged_dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488aaa46-8190-4c14-931a-d1f7e2951c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_data = {part: [] for part in partitions}\n",
    "keys_to_copy = ['text_duplicates', 'titles_supervised', 'raw_titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'review', 'valid', 'style_valid', 'duplicates']\n",
    "\n",
    "for part in partitions:\n",
    "    for entry in merged_dataset[part]:\n",
    "        reduced_entry = {k: entry[k] for k in keys_to_copy}\n",
    "        public_data[part].append(reduced_entry)\n",
    "        \n",
    "with open('data/huggingface_patch.pkl', 'wb') as handle:\n",
    "    pickle.dump(public_data, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904d1b1-8874-4654-9e3a-3229047c5a06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TODO: remove all this??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c0ebc664-7e10-43d5-b901-d74f38f6794d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_label = huggingface_label\n",
    "for key, split in merged_dataset.items():\n",
    "    for sample in split:\n",
    "        for title in sample['titles_supervised']:\n",
    "            if title not in merge_label:\n",
    "                merge_label.append(title)\n",
    "                \n",
    "merge_label.remove('')\n",
    "for title in merge_label:\n",
    "    title = title.replace('_',' ')\n",
    "    \n",
    "merge_label = list(set(merge_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6c8ae2c0-e3bf-43cb-ba30-3940860ea8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chiropractor',\n",
       " 'journalist',\n",
       " 'rapper',\n",
       " 'accountant',\n",
       " 'poet',\n",
       " 'surgeon',\n",
       " '*director/producer',\n",
       " 'architect',\n",
       " 'dentist',\n",
       " 'photographer',\n",
       " '*software architect',\n",
       " '*editor',\n",
       " 'professor',\n",
       " '*actor',\n",
       " 'model',\n",
       " 'composer',\n",
       " '*artist/ designer',\n",
       " 'comedian',\n",
       " 'painter',\n",
       " 'yoga teacher',\n",
       " '*entrepeneur',\n",
       " 'psychologist',\n",
       " '*writer',\n",
       " 'personal trainer',\n",
       " 'nurse',\n",
       " 'software_engineer',\n",
       " 'filmmaker',\n",
       " '*trainer',\n",
       " '*researcher',\n",
       " 'dj',\n",
       " 'interior_designer',\n",
       " 'dietitian',\n",
       " '*consultant/coaches',\n",
       " 'yoga_teacher',\n",
       " 'attorney',\n",
       " 'teacher',\n",
       " 'software engineer',\n",
       " 'pastor',\n",
       " 'physician',\n",
       " 'interior designer',\n",
       " 'paralegal',\n",
       " '*engineer',\n",
       " 'personal_trainer']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "682fe5b3-5e78-4a12-8a33-584b3e8b5bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_text She specialises in helping individuals manage their stress and anxiety and has developed and facilitated many groups on relaxation, stress and anxiety management. Katerina Volny BSc is a psychologist who has worked in public mental health and private practice settings in Melbourne, Australia. She is experienced in cognitive-behavioural techniques to treat a wide variety of individuals who experience stress and anxiety.\n",
      "profession 22\n",
      "gender 1\n",
      "text_duplicates []\n",
      "titles_supervised []\n",
      "raw_titles_supervised []\n",
      "gender_supervised F\n",
      "auto_title psychologist\n",
      "auto_raw_title psychologist\n",
      "path crawl-data/CC-MAIN-2016-44/segments/1476988719677.59/wet/CC-MAIN-20161020183839-00202-ip-10-171-6-4.ec2.internal.warc.wet.gz\n",
      "URI http://www.boomerangbooks.com.au/Relaxation-Techniques/Lillian-Nejad/book_9781845900786.htm\n",
      "review 1\n",
      "valid 0\n",
      "style_valid -1.0\n",
      "raw Lillian Nejad PhD is a clinical psychologist who has been working in the public mental health and community health systems in Melbourne, Australia for over ten years. She specialises in helping individuals manage their stress and anxiety and has developed and facilitated many groups on relaxation, stress and anxiety management. Katerina Volny BSc is a psychologist who has worked in public mental health and private practice settings in Melbourne, Australia. She is experienced in cognitive-behavioural techniques to treat a wide variety of individuals who experience stress and anxiety.\n",
      "bio _ specialises in helping individuals manage their stress and anxiety and has developed and facilitated many groups on relaxation, stress and anxiety management. Katerina Volny BSc is a psychologist who has worked in public mental health and private practice settings in Melbourne, Australia. _ is experienced in cognitive-behavioural techniques to treat a wide variety of individuals who experience stress and anxiety.\n",
      "name ('Lillian', '', 'Nejad')\n",
      "duplicates []\n"
     ]
    }
   ],
   "source": [
    "for elem in merged_dataset['train']:\n",
    "    if elem['review'] == 1 and elem['valid'] == 0:\n",
    "        for k, v in elem.items():\n",
    "            print(k,v)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "00661c2a-27df-4bb3-bb6b-6278b12c96a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mini_dataset = {key: [] for key in merged_dataset.keys()}\n",
    "\n",
    "for key, split in merged_dataset.items():\n",
    "    mini_dataset[key] = {k: [] for k in split[0].keys() if k not in ['duplicates','text_duplicates', 'raw_titles_supervised']}\n",
    "    for elem in merged_dataset[key]:\n",
    "        if elem['review'] == 0:\n",
    "            elem['titles_supervised'] = [elem['auto_title'].replace('_',' ')]\n",
    "        #    elem['raw_titles_supervised'] = [elem['auto_raw_title']]\n",
    "        #if elem['review'] == 1:\n",
    "        for k, v in elem.items():\n",
    "            if k in ['duplicates','text_duplicates', 'raw_titles_supervised']:#, 'raw_titles_supervised', 'titles_supervised']:\n",
    "                continue\n",
    "            if k in ['auto_title', 'auto_raw_title']:\n",
    "                mini_dataset[key][k].append(v.replace('_',' '))\n",
    "            else:\n",
    "                mini_dataset[key][k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9f1e3beb-d917-44f0-b063-74092b01de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(mini_dataset['train']))\n",
    "print(len(mini_dataset['test']))\n",
    "print(len(mini_dataset['dev']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "493c4f91-528a-4d14-8f92-28154932b921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, DatasetInfo\n",
    "info = DatasetInfo({'label': merge_label, 'features': mini_dataset['train'].keys(), 'num_rows': len(mini_dataset['train']['hard_text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "df2030d2-63a0-4ffe-bf55-00f800b16784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, DatasetInfo\n",
    "\n",
    "mdata_train = Dataset.from_dict(mini_dataset['train'], split='train')\n",
    "mdata_test = Dataset.from_dict(mini_dataset['test'], split='test')\n",
    "mdata_dev = Dataset.from_dict(mini_dataset['dev'], split='dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ffd1a712-b3a8-4eed-a119-9c3b4b995176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdata = DatasetDict({'train': mdata_train, 'test': mdata_test, 'dev': mdata_dev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "228ade72-e03c-4dc5-bc8f-8267f3c13dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 257478\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 99069\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 39642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d12c1fe4-d9bd-4144-bb1b-aa31c364d05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f450f7c7cf4656b5482be45f669251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/258 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce36403065447658c3a33190f77f4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693de4f3f4b442a98a08b3f0f5a97c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in ['train', 'test', 'dev']:\n",
    "    mdata[split].to_csv(split+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c55bfe-4df1-4e08-b118-7ff5b77ff284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6cae45bd-a1da-4bd2-ae01-ec810729f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/saschroeder/.cache/huggingface/datasets/csv/default-2ae8139e51537722/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70556632b11047e39d4cb6c072438d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acaae70b3f24e369cd4e751c0353a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/saschroeder/.cache/huggingface/datasets/csv/default-2ae8139e51537722/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/saschroeder/.cache/huggingface/datasets/csv/default-1ee6525d50e0d906/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726bd0d20e454d95a3f0baff46cfff6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b0e87dcebb4eeb8f901d97df66770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/saschroeder/.cache/huggingface/datasets/csv/default-1ee6525d50e0d906/0.0.0. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /home/saschroeder/.cache/huggingface/datasets/csv/default-c2edacc0f48fec01/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05eff183243e43c5bd26652257e6316b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ca2a2dcc4f474398d0c52a8e70705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/saschroeder/.cache/huggingface/datasets/csv/default-c2edacc0f48fec01/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "mdata = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    mdata[split] = Dataset.from_csv(split+'.csv')\n",
    "    \n",
    "dataset = DatasetDict(mdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ca25f681-ec8c-4c57-bdea-cd26cf960b23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 257483\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 99070\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 39642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1fc5bdb9-846f-4c65-a3f0-fe755e637a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_text': 'He is also the project lead of and major contributor to the open source assembler/simulator \"EASy68K.\" He earned a master’s degree in computer science from the University of Michigan-Dearborn, where he is also an adjunct instructor. Downloads/Updates',\n",
       " 'profession': 21,\n",
       " 'gender': 0,\n",
       " 'titles_supervised': \"['professor']\",\n",
       " 'gender_supervised': 'M',\n",
       " 'auto_title': 'professor',\n",
       " 'auto_raw_title': 'associate professor',\n",
       " 'path': 'crawl-data/CC-MAIN-2016-44/segments/1476988720615.90/wet/CC-MAIN-20161020183840-00099-ip-10-171-6-4.ec2.internal.warc.wet.gz',\n",
       " 'URI': 'https://www.crcpress.com/Programming-2D-Games/Kelly/p/book/9781466508682',\n",
       " 'review': 0,\n",
       " 'valid': -1,\n",
       " 'style_valid': -1.0,\n",
       " 'raw': 'Charles Kelly is an associate professor at Monroe County Community College, where he teaches game programming and other computer science courses. He is also the project lead of and major contributor to the open source assembler/simulator \"EASy68K.\" He earned a master’s degree in computer science from the University of Michigan-Dearborn, where he is also an adjunct instructor. Downloads/Updates',\n",
       " 'bio': '_ is also the project lead of and major contributor to the open source assembler/simulator \"EASy68K.\" _ earned a master’s degree in computer science from the University of Michigan-Dearborn, where _ is also an adjunct instructor. Downloads/Updates',\n",
       " 'name': \"['Charles' '' 'Kelly']\"}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed202ab5-0d32-424a-8333-e77fc320c6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1f6bf0a2-4bfe-4cda-8787-e38a88b9bbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/saschroeder/.cache/huggingface/datasets/csv/default-2ae8139e51537722/0.0.0)\n",
      "Found cached dataset csv (/home/saschroeder/.cache/huggingface/datasets/csv/default-1ee6525d50e0d906/0.0.0)\n",
      "Found cached dataset csv (/home/saschroeder/.cache/huggingface/datasets/csv/default-c2edacc0f48fec01/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mdata = {}\n",
    "for split in ['train', 'test', 'dev']:\n",
    "    mdata[split] = Dataset.from_csv(split+'.csv')\n",
    "    \n",
    "dataset = DatasetDict(mdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3f754867-fb42-43fd-988c-35071865e416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 257483\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 99070\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'path', 'URI', 'review', 'valid', 'style_valid', 'raw', 'bio', 'name'],\n",
       "        num_rows: 39642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e965cbcf-1694-42df-8e81-8e556f249c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard_text': ' She has already put up more than three years in the field of teaching and has remained a hard working and outstanding student through out her career without any external support or help of any sort and secured first Division in all her career pursuits. She has respect for elders and a strong faith in  traditional values of the family. ', 'profession': None, 'gender': None, 'titles_supervised': None, 'gender_supervised': None, 'auto_title': None, 'auto_raw_title': None, 'path': None, 'URI': None, 'review': None, 'valid': None, 'style_valid': None, 'raw': None, 'bio': None, 'name': None}\n",
      "{'hard_text': 'She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job. ', 'profession': None, 'gender': None, 'titles_supervised': None, 'gender_supervised': None, 'auto_title': None, 'auto_raw_title': None, 'path': None, 'URI': None, 'review': None, 'valid': None, 'style_valid': None, 'raw': None, 'bio': None, 'name': None}\n",
      "{'hard_text': 'Her work includes photography and writing on various elements of Hong Kong culture including the wealth of traditional and modern storylines that collide on a daily basis. ', 'profession': None, 'gender': None, 'titles_supervised': None, 'gender_supervised': None, 'auto_title': None, 'auto_raw_title': None, 'path': None, 'URI': None, 'review': None, 'valid': None, 'style_valid': None, 'raw': None, 'bio': None, 'name': None}\n",
      "{'hard_text': 'She was also present in Cairo in early 2013 and witnessed the huge demonstrations against Morsi in Tahrir Square and was there for the second anniversary of the revolution.', 'profession': None, 'gender': None, 'titles_supervised': None, 'gender_supervised': None, 'auto_title': None, 'auto_raw_title': None, 'path': None, 'URI': None, 'review': None, 'valid': None, 'style_valid': None, 'raw': None, 'bio': None, 'name': None}\n",
      "{'hard_text': 'Normally she covers the Suburban Beat but caught a break one evening when the police beat story came in and the regular police reporter was still working on another project.', 'profession': None, 'gender': None, 'titles_supervised': None, 'gender_supervised': None, 'auto_title': None, 'auto_raw_title': None, 'path': None, 'URI': None, 'review': None, 'valid': None, 'style_valid': None, 'raw': None, 'bio': None, 'name': None}\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset['train']:\n",
    "    if elem['titles_supervised'] is None:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbf268-b49b-4612-9b60-b1ebd5307e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24dc851b-68a4-4e06-8db6-97a0c9229cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TODO:\n",
    "- (x) save pkl with all info\n",
    "- (x) save pkl with additional info for huggingface (without privacy/copyrigth issues)\n",
    "- code example to transform to filter and transform to huggingface dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a0b1a9e4-b7fd-419a-838f-5ea8373e5226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('merged.pkl', 'rb') as handle:\n",
    "    merged_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f6c8a841-9491-405f-a04a-35c030adab0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "KEYS_TO_COPY = ['hard_text', 'profession', 'gender', 'raw', 'titles_supervised', 'review', 'valid', 'name']\n",
    "\n",
    "def filter_dataset(dataset: dict, classes: list, single_label=True, review_only=True, valid_only=True):\n",
    "    splits = dataset.keys()\n",
    "    split_dict = {}\n",
    "    filtered_dataset = {split: [] for split in splits}\n",
    "    for split in splits:\n",
    "        for elem in dataset[split]:\n",
    "            if valid_only and elem['valid'] != 1:\n",
    "                continue\n",
    "            if review_only and elem['review'] != 1:\n",
    "                continue\n",
    "            sel_titles = [title for title in elem['titles_supervised'] if title in classes]\n",
    "            if single_label and len(sel_titles) > 1:\n",
    "                continue\n",
    "            if len(sel_titles) == 0:\n",
    "                continue\n",
    "                \n",
    "            new_entry = {k: elem[k] for k in KEYS_TO_COPY}\n",
    "            if single_label:\n",
    "                label = classes.index(sel_titles[0])\n",
    "            else: # multi-label / one-hot encoded\n",
    "                label = np.ones(len(classes))\n",
    "                for title in sel_titles:\n",
    "                    label[classes.index(title)] = 1\n",
    "            new_entry.update({'label': label})\n",
    "            filtered_dataset[split].append(new_entry)\n",
    "        print(len(filtered_dataset[split]))\n",
    "        \n",
    "        cur_split = {k: [elem[k] for elem in filtered_dataset[split]] for k in filtered_dataset[split][0].keys()}\n",
    "        split_dict[split] = Dataset.from_dict(cur_split, split=split)\n",
    "    return DatasetDict(split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8a4fb8ca-8eb0-4a84-8268-d47d5d841200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6855\n",
      "2430\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "classes = ['architect', 'surgeon', 'dentist', 'teacher', 'psychologist', 'nurse', 'photographer', 'physician', 'attorney', 'journalist']\n",
    "\n",
    "fdata = filter_dataset(merged_dataset, classes, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f6336e21-d81d-4941-8d26-9a6d2fc9b888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'raw', 'titles_supervised', 'review', 'valid', 'name', 'label'],\n",
       "        num_rows: 7017\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'raw', 'titles_supervised', 'review', 'valid', 'name', 'label'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['hard_text', 'profession', 'gender', 'raw', 'titles_supervised', 'review', 'valid', 'name', 'label'],\n",
       "        num_rows: 1046\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8643c309-c71f-4f57-940e-32813718f910",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_text': ['She graduated in May 2008 from Corban College with a Bachelor of Science in English-Journalism. She currently works as the Office Administrator at her church. Shawnee has been working along side her husband in youth ministry for four and a half years.',\n",
       "  \"Alderfer categorized the lower order needs (Physiological and Safety) into the Existence category. He fit Maslow's interpersonal love and esteem needs into the relatedness category. The growth category contained the Self Actualization and self esteem needs. Alderfer also proposed a regression theory to go along with the ERG theory. He said that when needs in a higher category are not met then individuals redouble the efforts invested in a lower category need. For example if self actualization or self esteem is not met then individuals will invest more effort in the relatedness category in the hopes of achieving the higher need.\"],\n",
       " 'profession': [11, 22],\n",
       " 'gender': [1, 0],\n",
       " 'raw': ['Shawnee Randolph is a freelance journalist who lives with her husband near Salem, Ore. She graduated in May 2008 from Corban College with a Bachelor of Science in English-Journalism. She currently works as the Office Administrator at her church. Shawnee has been working along side her husband in youth ministry for four and a half years.',\n",
       "  \"Clayton Paul Alderfer is an American psychologist who further expanded Maslow's hierarchy of needs by categorizing the hierarchy into his ERG theory of motivation (Existence, Relatedness and Growth). Alderfer categorized the lower order needs (Physiological and Safety) into the Existence category. He fit Maslow's interpersonal love and esteem needs into the relatedness category. The growth category contained the Self Actualization and self esteem needs. Alderfer also proposed a regression theory to go along with the ERG theory. He said that when needs in a higher category are not met then individuals redouble the efforts invested in a lower category need. For example if self actualization or self esteem is not met then individuals will invest more effort in the relatedness category in the hopes of achieving the higher need.\"],\n",
       " 'titles_supervised': [['journalist'], ['psychologist']],\n",
       " 'review': [1, 1],\n",
       " 'valid': [1, 1],\n",
       " 'name': [['Shawnee', '', 'Randolph'], ['Clayton', 'Paul', 'Alderfer']],\n",
       " 'label': [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44a2ad-9742-4f89-8ebb-f10228ed60c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
