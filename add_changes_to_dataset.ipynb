{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-electricity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-vegetable",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIOS_FILE_REVIEWED = \"data/BIOS_REVIEWED.pkl\" # only reviewed samples\n",
    "BIOS_FILE_COMPLETE = \"data/BIOS_COMPLETE.pkl\" # whole dataset with changes to the reviewed samples\n",
    "BIOS_LABELS = \"data/BIOS_LABELS.csv\" # this contains all of our changes without the sensitive data, but ids matching the BIOS dataset\n",
    "BIOS_RAW = \"../../data/biosbias2023/BIOS.pkl\" # \"data/BIOS.pkl\"\n",
    "TITLE_JSON = \"data/title_lookup.json\"\n",
    "\n",
    "reviewed_classes = ['architect', 'surgeon', 'dentist', 'teacher', 'psychologist', 'nurse', 'photographer', 'physician', 'attorney', 'journalist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c44fd-d0db-4368-9eab-a950ae190832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with (open(BIOS_RAW, \"rb\")) as openfile:\n",
    "    raw_data = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-columbia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv(BIOS_LABELS, sep='\\t')\n",
    "labeled_data = df_label.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-hybrid",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(labeled_data)):\n",
    "    if not (raw_data[i]['raw_title'] == labeled_data[i]['auto_raw_title'] and raw_data[i]['start_pos'] == labeled_data[i]['start_pos']):\n",
    "        print(\"data mismatch at id \", i)\n",
    "        \n",
    "    labeled_data[i]['raw'] = raw_data[i]['raw']\n",
    "    labeled_data[i]['bio'] = raw_data[i]['bio']\n",
    "    labeled_data[i]['name'] = raw_data[i]['name']\n",
    "    \n",
    "    # convert titles from nan/string to list\n",
    "    if type(labeled_data[i]['titles']) == float:\n",
    "        labeled_data[i]['titles'] = []\n",
    "    elif type(labeled_data[i]['titles']) == str:\n",
    "        titles = labeled_data[i]['titles'].strip(']\"\\'[').split('\\', \\'')\n",
    "        labeled_data[i]['titles'] = titles\n",
    "        \n",
    "    # convert raw titles fron nan/string to list\n",
    "    if type(labeled_data[i]['raw_titles']) == float:\n",
    "        labeled_data[i]['raw_titles'] = []\n",
    "    elif type(labeled_data[i]['titles']) == str:\n",
    "        titles = labeled_data[i]['raw_titles'].strip(']\"\\'[').split(', ')\n",
    "        labeled_data[i]['raw_titles'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-dealing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=labeled_data)\n",
    "df_reviewed = df[df['review'] == 1]\n",
    "reviewed_data = df_reviewed.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-timeline",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with (open(BIOS_FILE_COMPLETE, \"wb\")) as openfile:\n",
    "    pickle.dump(labeled_data, openfile)\n",
    "\n",
    "with (open(BIOS_FILE_REVIEWED, \"wb\")) as openfile:\n",
    "    pickle.dump(reviewed_data, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7353a9-03ec-4c62-9028-828b9c3a0221",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Update: Prepare the reviewed data for merging with huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931712d-f745-42c9-bb64-91ddf56c1ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "\n",
    "# labels in the order specified here: https://huggingface.co/datasets/LabHC/bias_in_bios\n",
    "# not included in the datasets metadata\n",
    "huggingface_label = ['accountant', 'architect', 'attorney', 'chiropractor', 'comedian', 'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', 'interior_designer', 'journalist', 'model', 'nurse', 'painter', 'paralegal', 'pastor', 'personal_trainer', 'photographer', 'physician', 'poet', 'professor', 'psychologist', 'rapper', 'software_engineer', 'surgeon', 'teacher', 'yoga_teacher']\n",
    "partitions = ['train', 'test', 'dev']\n",
    "\n",
    "dataset['train'].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cfae8-f7c7-4847-bee6-b6b5a5adba17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in labeled_data:\n",
    "    sample['match_text'] = sample['raw'][sample['start_pos']:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f2c41-dad4-4393-9ce7-4ab4a4c40bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MATCH_PKL = 'matches.pkl'\n",
    "if not os.path.isfile(MATCH_PKL):\n",
    "    matches = {part: [] for part in partitions}\n",
    "    reverse_matches = [[] for sample in labeled_data]\n",
    "\n",
    "    for part in partitions:\n",
    "        matches[part] = []\n",
    "        print(part)\n",
    "        for i in tqdm(range(len(dataset[part]))):\n",
    "            text = dataset[part][i]['hard_text']\n",
    "            f = list(filter(lambda labeled_data: labeled_data['match_text'] == text, labeled_data))\n",
    "            potential_matches = []\n",
    "            for elem in f:\n",
    "                if elem['auto_title'] == huggingface_label[dataset[part][i]['profession']]:\n",
    "                    potential_matches.append(elem['Unnamed: 0'])\n",
    "                    reverse_matches[elem['Unnamed: 0']].append((part,i))\n",
    "            matches[part].append((i,potential_matches))\n",
    "\n",
    "    with open(MATCH_PKL, 'wb') as handle:\n",
    "        pickle.dump({'matches': matches, 'reverse_matches': reverse_matches}, handle)\n",
    "    \n",
    "else:\n",
    "    print(\"load precomputed matches\")\n",
    "    with open(MATCH_PKL, 'rb') as handle:\n",
    "        loaded_matches = pickle.load(handle)\n",
    "        matches = loaded_matches['matches']\n",
    "        reverse_matches = loaded_matches['reverse_matches']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38f56c-11f4-4a6c-8259-7ff270277e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for part in partitions:\n",
    "    print(part)\n",
    "    count0 = 0\n",
    "    count_double = 0\n",
    "\n",
    "    for match in matches[part]:\n",
    "        if match[1] == []:\n",
    "            count0 += 1\n",
    "        elif len(match[1]) > 1:\n",
    "            count_double += 1\n",
    "\n",
    "    print(\"got %i samples in total\" % len(matches[part]))\n",
    "    print(\"for %i samples found no match in the labeled data\" % count0)\n",
    "    print(\"for %i samples found multiple matches in the labeled data\" % count_double)\n",
    "    \n",
    "\n",
    "count0 = 0\n",
    "count_multi = 0\n",
    "for match in reverse_matches:\n",
    "    n_matches = len(match)\n",
    "    if n_matches == 0:\n",
    "        count0 += 1\n",
    "    if n_matches > 1:\n",
    "        count_multi += 1\n",
    "        \n",
    "print(\"based on the crawled dataset\")\n",
    "print(\"got %i samples in total\" % len(reverse_matches))\n",
    "print(\"for %i samples found no match in the labeled data\" % count0)\n",
    "print(\"for %i samples found multiple matches in the labeled data\" % count_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd0884-75f2-47f1-89d1-a382f78ed679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_for_filter = []\n",
    "for part in partitions:\n",
    "    for i, sample in enumerate(dataset[part]):\n",
    "        sample_for_filter = sample.copy()\n",
    "        sample_for_filter.update({'split': part, 'id': i})\n",
    "        dataset_for_filter.append(sample_for_filter)\n",
    "        \n",
    "dataset_for_filter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36daf615-b6ca-4cdf-9cee-6f9c97f66595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_dataset = {part: [] for part in partitions}\n",
    "dup_id_list = []\n",
    "\n",
    "for part in partitions:\n",
    "    print(part)\n",
    "    for match in tqdm(matches[part]):\n",
    "        idx = match[0]\n",
    "        sample = dataset[part][idx]\n",
    "        text = sample['hard_text']\n",
    "\n",
    "        # duplicates in the huggingface dataset:\n",
    "        f = list(filter(lambda dataset_for_filter: dataset_for_filter['hard_text'] == text, dataset_for_filter))\n",
    "        sample.update({'text_duplicates': [(elem['split'], elem['id']) for elem in f if (elem['id'] != idx or elem['split'] != part)]})\n",
    "\n",
    "        if len(match[1]) == 0:\n",
    "            # keep old label\n",
    "            sample.update({'titles_supervised': [], 'raw_titles_supervised': [], 'gender_supervised': '', 'auto_title': '', 'auto_raw_title': '', 'path': '', 'URI': '', \n",
    "                           'review': 0, 'valid': -1, 'style_valid': -1, 'raw': '', 'bio': '', 'name': ('',''), 'duplicates': []})\n",
    "            merged_dataset[part].append(sample)\n",
    "        elif len(match[1]) > 1:\n",
    "            dup_id_list.append(match[0])\n",
    "            duplicates = []\n",
    "            for m_id in match[1]:\n",
    "                for d_tup in reverse_matches[m_id]:\n",
    "                    if d_tup[1] != idx and d_tup not in duplicates:\n",
    "                        duplicates.append(d_tup)\n",
    "\n",
    "            same_number_of_duplicates = (len(duplicates)+1 == len(match[1]))\n",
    "            all_equal = True\n",
    "            if not same_number_of_duplicates:\n",
    "                base_sample = labeled_data[match[1][0]]\n",
    "                #if len(duplicates) > 0 and len(match[1]) > 1:\n",
    "                #    print(\"inconsistent number of duplicates: %i vs. %i\" % (len(duplicates)+1, len(match[1])))\n",
    "                #    print(idx, match[1][0], \"base sample: \")\n",
    "                #    print_sample(base_sample)\n",
    "                for next_id in match[1][1:]:\n",
    "                    comp_sample = labeled_data[next_id]\n",
    "                    if not (comp_sample['titles'] == base_sample['titles'] and comp_sample['gender'] == base_sample['gender']):\n",
    "                        all_equal = False\n",
    "                assert all_equal, \"inconsistent number of duplicates in both dataset versions\"+str(match[1])+ \" vs. \" + str(duplicates)+\" for idx \"+str(idx)\n",
    "\n",
    "                # take first possible match\n",
    "                match_id = match[1][0]\n",
    "                merge_sample = labeled_data[match_id]\n",
    "\n",
    "                # assign other match ids to duplicates (1:1), handle inconsistent numbers of duplicates!\n",
    "                for i, (d_split,d_idx) in enumerate(duplicates):\n",
    "                    if i+1 >= len(matches[d_split][d_idx][1]):\n",
    "                        i -= len(matches[d_split][d_idx][1])\n",
    "                    matches[d_split][d_idx] = (matches[d_split][d_idx][0], [matches[d_split][d_idx][1][i+1]])\n",
    "                    #print(d_split, d_idx, matches[d_split][d_idx][1], \"->\", [matches[d_split][d_idx][1][i]])\n",
    "            else:\n",
    "                # take first possible match\n",
    "                match_id = match[1][0]\n",
    "                merge_sample = labeled_data[match_id]\n",
    "\n",
    "                # assign other match ids to duplicates (1:1)\n",
    "                for i, (d_split,d_idx) in enumerate(duplicates):\n",
    "                    matches[d_split][d_idx] = (matches[d_split][d_idx][0], [matches[d_split][d_idx][1][i+1]])\n",
    "                    #print(matches[d_split][d_idx][1], \"->\", [matches[d_split][d_idx][1][i]])\n",
    "\n",
    "            sample.update({'titles_supervised': merge_sample['titles'], 'raw_titles_supervised': merge_sample['raw_titles'], 'gender_supervised': merge_sample['gender'], \n",
    "                           'auto_title': merge_sample['auto_title'], 'auto_raw_title': merge_sample['auto_raw_title'],  'path': merge_sample['path'], 'URI': merge_sample['URI'], \n",
    "                           'review': merge_sample['review'], 'valid': merge_sample['valid'], 'style_valid': merge_sample['style_valid'], 'raw': merge_sample['raw'], \n",
    "                           'bio': merge_sample['bio'], 'name': merge_sample['name'], 'duplicates': duplicates})\n",
    "            merged_dataset[part].append(sample)\n",
    "\n",
    "        else:\n",
    "            match_id = match[1][0]\n",
    "            merge_sample = labeled_data[match_id]\n",
    "            sample.update({'titles_supervised': merge_sample['titles'], 'raw_titles_supervised': merge_sample['raw_titles'], 'gender_supervised': merge_sample['gender'], \n",
    "                           'auto_title': merge_sample['auto_title'], 'auto_raw_title': merge_sample['auto_raw_title'],  'path': merge_sample['path'], 'URI': merge_sample['URI'], \n",
    "                           'review': merge_sample['review'], 'valid': merge_sample['valid'], 'style_valid': merge_sample['style_valid'], 'raw': merge_sample['raw'], \n",
    "                           'bio': merge_sample['bio'], 'name': merge_sample['name'], 'duplicates': []})\n",
    "            merged_dataset[part].append(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52727fdc-45a0-4e58-9166-fe68e63c19d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(merged_dataset[part]):\n",
    "    if len(sample['duplicates']) > 0:\n",
    "        for (split, idx) in sample['duplicates']:\n",
    "            if len(merged_dataset[split][idx]['duplicates']) == 0:\n",
    "                dup = [tup for tup in sample['duplicates'] if not tup == (split,idx)]\n",
    "                dup.append((part, i))\n",
    "                merged_dataset[split][idx]['duplicates'] = dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2a00a-152d-4094-b45a-2b60cc333ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/huggingface_merge_complete.pkl', 'wb') as handle:\n",
    "    pickle.dump(merged_dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488aaa46-8190-4c14-931a-d1f7e2951c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_data = {part: [] for part in partitions}\n",
    "keys_to_copy = ['text_duplicates', 'titles_supervised', 'raw_titles_supervised', 'gender_supervised', 'auto_title', 'auto_raw_title', 'review', 'valid', 'style_valid', 'duplicates']\n",
    "\n",
    "for part in partitions:\n",
    "    for entry in merged_dataset[part]:\n",
    "        reduced_entry = {k: entry[k] for k in keys_to_copy}\n",
    "        public_data[part].append(reduced_entry)\n",
    "        \n",
    "with open('data/huggingface_patch.pkl', 'wb') as handle:\n",
    "    pickle.dump(public_data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44a2ad-9742-4f89-8ebb-f10228ed60c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
